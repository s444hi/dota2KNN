import numpy as np # fast arrays, math operations, manipulating arrays
import matplotlib.pyplot as plt # main plotting library, used for histograms, scatter plots, subplots
import pandas as pd # handles tabular data, loads csv into dataframe and grab feature/label columns
from sklearn.preprocessing import StandardScaler # scales fts to mean 0, variance 1
from sklearn.neighbors import KNeighborsClassifier, NearestCentroid # distance based classifiers
from sklearn.model_selection import train_test_split # splits data to training and testing
from sklearn.inspection import DecisionBoundaryDisplay # helps visualize classifier boundaries
from sklearn.metrics import accuracy_score # how well predictions match true labels

from google.colab import drive
drive.mount('/content/drive')

url = 'https://docs.google.com/spreadsheets/d/1GT0vNzFgCKWwnXHrz3ruC71f8W2qWxJ7nLNMGsfQlMY/export?format=csv'
df = pd.read_csv(url)

# mount to gd to access files, pandas downloads sheet --> exported as csv to dataframe df

df.head()

X = df.iloc[:, :-1]
y = df.iloc[:, -1].astype(int)

# pandas indexing iloc slices df my row/column, x = win rate, pick rate, kda, y = hero label (abaddon, wraith king, warlock)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# converts features so they're ont the same scale because if it doesn't then knn and nc
# would be skwed

fig, axes = plt.subplots(1, 4, figsize=(12, 3))

for i in range(4):
  axes[i].hist(df.iloc[:,i], color='navy')
  axes[i].set_title(f"feature {i}")

  fig.tight_layout()

  # matplotlib makes histograms of first 4 features to check distributions
  # ft 0 = win rate, ft 1 = pick rate, ft 2 = kda, ft 3 = game mode

fig, axes = plt.subplots(4,4,figsize=(8,8))

for i in range(4):
    for j in range(4):
        axes[i, j].set_xlabel(f"feature {i}")
        axes[i, j].set_ylabel(f"feature {j}")

        if j == i:
            axes[i, j].hist(df.iloc[:, i], color='navy')
        else:
            for k in range(len(df)):
                if df.iloc[k]['Hero'] == 1:
                    axes[i, j].scatter(df.iloc[k, j], df.iloc[k, i], color='maroon', s=10)
                elif df.iloc[k]['Hero'] == 2:
                    axes[i, j].scatter(df.iloc[k, j], df.iloc[k, i], color='navy', s=10)
                else:
                    axes[i, j].scatter(df.iloc[k, j], df.iloc[k, i], color='orange', s=10)

fig.tight_layout()

# matplotlib creates 4x4 grid of plots, diagonal = histograms, rest = scatter of one ft vs another colored by class
# red: abaddon, blue: wraith king, yellow: warlock

# we can see that the red is highest for all the fts


seed = 1234
# random seed fixes the sequence of randon nums so that the experiment is reproducible
# so if i were to run this again i'll get the same train/test split instead of a new one every time
np.random.seed(seed) # tells numpy to use the chosen seed whenever there's randomness like shuffling
# without this results could vary run to run
Xtr, Xte, Ytr, Yte = train_test_split(X_scaled, y, test_size=0.3, random_state=seed, shuffle=True)
# breaks dataset into training data (for learnign) and test data (for evaluation)

# xscaled = fts (win rate, pick rate, kda, game mode)
# y = labels (abaddon, wraith king, warlock
# shuffle = true shuffles the rwos before splitting so the data isn't grouped by order

# Xtr: training features, Xte: testing features, Ytr: training labels, Yte: testing labels
# so the classifier is trained on xtr ytr and tested on xte, yte

NC = NearestCentroid()
NC.fit(Xtr, Ytr)
Pred_tr = NC.predict(Xtr)
Pred_te = NC.predict(Xte)
Accuracy_tr = accuracy_score(Ytr, Pred_tr)
Accuracy_te = accuracy_score(Yte, Pred_te)

print(f"-- accuracy (train): {Accuracy_tr}")
print(f"-- accuracy (test): {Accuracy_te}")

// -- accuracy (train): 0.8648648648648649
// -- accuracy (test): 0.8160919540229885

plot_kwargs = {'cmap' : 'viridis',
               'response_method' : 'predict',
               'plot_method' : 'pcolormesh',
               'shading' : 'auto',
               'alpha' : 0.6,
               'grid_resolution' : 100}

figure, axes = plt.subplots(2, 2, figsize=(10, 10))

k_vals = [1, 5, 10, 37] 
for i, k in enumerate(k_vals):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(Xtr[:, :2], Ytr)

    DecisionBoundaryDisplay.from_estimator(knn, Xtr[:, :2], ax=axes[i // 2, i % 2], **plot_kwargs)
    axes[i // 2, i % 2].scatter(Xtr[:, 0], Xtr[:, 1], c=Ytr, edgecolor='k', s=12)
    axes[i // 2, i % 2].set_title(f"k: {k}")

plt.tight_layout()
plt.show()




from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import NearestCentroid
from sklearn.metrics import accuracy_score

param_grid_nc = {'shrink_threshold': [None, 0.1, 0.5, 1.0, 2.0]}

grid_search_nc = GridSearchCV(NearestCentroid(), param_grid_nc, cv=5)

grid_search_nc.fit(Xtr, Ytr)

best_params_nc = grid_search_nc.best_params_
best_score_nc = grid_search_nc.best_score_

print(f"Best hyperparameters for Nearest Centroid: {best_params_nc}")
print(f"Best cross-validation accuracy for Nearest Centroid: {best_score_nc}")

best_nc = NearestCentroid(shrink_threshold=best_params_nc['shrink_threshold'])
best_nc.fit(Xtr, Ytr)
Pred_te_tuned_nc = best_nc.predict(Xte)
Accuracy_te_tuned_nc = accuracy_score(Yte, Pred_te_tuned_nc)

print(f"Test accuracy with tuned Nearest Centroid: {Accuracy_te_tuned_nc}")


Best hyperparameters for Nearest Centroid: {'shrink_threshold': 1.0}
Best cross-validation accuracy for Nearest Centroid: 1.0
Test accuracy with tuned Nearest Centroid: 0.9425287356321839
